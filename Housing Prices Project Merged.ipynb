{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "3fd09b2c-f2e7-46e8-8ff4-04ec5e5c398a",
   "metadata": {},
   "source": [
    "## Import the relevant libraries\n",
    "\n",
    "import pandas as pd \n",
    "import numpy as np \n",
    "import plotly.graph_objects as go\n",
    "import plotly.express as px\n",
    "import scipy.stats as stats\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "## Basic data exploration\n",
    "\n",
    "1. Import the data\n",
    "2. Look at summary statisitcs\n",
    "3. Evaluate Null Values\n",
    "\n",
    "#import data \n",
    "df = pd.read_csv(\"/Users/achalas/Documents/Housing Prices Project/house-prices-advanced-regression-techniques/train.csv\")\n",
    "\n",
    "# Function to create scrollable table within a small window\n",
    "def create_scrollable_table(df, table_id, title):\n",
    "    html = f'<h3>{title}</h3>'\n",
    "    html += f'<div id=\"{table_id}\" style=\"height:200px; overflow:auto;\">'\n",
    "    html += df.to_html()\n",
    "    html += '</div>'\n",
    "    return html\n",
    "\n",
    "df.shape\n",
    "\n",
    "numerical_features = df.select_dtypes(include=[np.number])\n",
    "numerical_features.describe()\n",
    "\n",
    "# Summary statistics for numerical features\n",
    "numerical_features = df.select_dtypes(include=[np.number])\n",
    "summary_stats = numerical_features.describe().T\n",
    "html_numerical = create_scrollable_table(summary_stats, 'numerical_features', 'Summary statistics for numerical features')\n",
    "\n",
    "display(HTML(html_numerical))\n",
    "\n",
    "# Summary statistics for categorical features\n",
    "categorical_features = df.select_dtypes(include=[object])\n",
    "cat_summary_stats = categorical_features.describe().T\n",
    "html_categorical = create_scrollable_table(cat_summary_stats, 'categorical_features', 'Summary statistics for categorical features')\n",
    "\n",
    "display(HTML(html_categorical ))\n",
    "\n",
    "# Null values in the dataset\n",
    "null_values = df.isnull().sum()\n",
    "html_null_values = create_scrollable_table(null_values.to_frame(), 'null_values', 'Null values in the dataset')\n",
    "\n",
    "# Percentage of missing values for each feature\n",
    "missing_percentage = (df.isnull().sum() / len(df)) * 100\n",
    "html_missing_percentage = create_scrollable_table(missing_percentage.to_frame(), 'missing_percentage', 'Percentage of missing values for each feature')\n",
    "\n",
    "display(HTML(html_null_values + html_missing_percentage))\n",
    "\n",
    "# Exploring rows with missing values\n",
    "rows_with_missing_values = df[df.isnull().any(axis=1)]\n",
    "html_rows_with_missing_values = create_scrollable_table(rows_with_missing_values.head(), 'rows_with_missing_values', 'Rows with missing values')\n",
    "\n",
    "display(HTML(html_rows_with_missing_values))\n",
    "\n",
    "df.columns\n",
    "\n",
    "## Explore the dependent variable\n",
    "\n",
    "import scipy.stats as stats\n",
    "\n",
    "# Fit a normal distribution to the SalePrice data\n",
    "mu, sigma = stats.norm.fit(df['SalePrice'])\n",
    "\n",
    "# Create a histogram of the SalePrice column\n",
    "hist_data = go.Histogram(x=df['SalePrice'], nbinsx=50, name=\"Histogram\", opacity=0.75, histnorm='probability density', marker=dict(color='purple'))\n",
    "\n",
    "# Calculate the normal distribution based on the fitted parameters\n",
    "x_norm = np.linspace(df['SalePrice'].min(), df['SalePrice'].max(), 100)\n",
    "y_norm = stats.norm.pdf(x_norm, mu, sigma)\n",
    "\n",
    "# Create the normal distribution overlay\n",
    "norm_data = go.Scatter(x=x_norm, y=y_norm, mode=\"lines\", name=f\"Normal dist. (μ={mu:.2f}, σ={sigma:.2f})\", line=dict(color=\"green\"))\n",
    "\n",
    "# Combine the histogram and the overlay\n",
    "fig = go.Figure(data=[hist_data, norm_data])\n",
    "\n",
    "# Set the layout for the plot\n",
    "fig.update_layout(\n",
    "    title=\"SalePrice Distribution\",\n",
    "    xaxis_title=\"SalePrice\",\n",
    "    yaxis_title=\"Density\",\n",
    "    legend_title_text=\"Fitted Normal Distribution\",\n",
    "    plot_bgcolor='rgba(32, 32, 32, 1)',\n",
    "    paper_bgcolor='rgba(32, 32, 32, 1)',\n",
    "    font=dict(color='white')\n",
    ")\n",
    "\n",
    "# Create a Q-Q plot\n",
    "qq_data = stats.probplot(df['SalePrice'], dist=\"norm\")\n",
    "qq_fig = px.scatter(x=qq_data[0][0], y=qq_data[0][1], labels={'x': 'Theoretical Quantiles', 'y': 'Ordered Values'}, color_discrete_sequence=[\"purple\"])\n",
    "qq_fig.update_layout(\n",
    "    title=\"Q-Q plot\",\n",
    "    plot_bgcolor='rgba(32, 32, 32, 1)',\n",
    "    paper_bgcolor='rgba(32, 32, 32, 1)',\n",
    "    font=dict(color='white')\n",
    ")\n",
    "\n",
    "# Calculate the line of best fit\n",
    "slope, intercept, r_value, p_value, std_err = stats.linregress(qq_data[0][0], qq_data[0][1])\n",
    "line_x = np.array(qq_data[0][0])\n",
    "line_y = intercept + slope * line_x\n",
    "\n",
    "# Add the line of best fit to the Q-Q plot\n",
    "line_data = go.Scatter(x=line_x, y=line_y, mode=\"lines\", name=\"Normal Line\", line=dict(color=\"green\"))\n",
    "\n",
    "# Update the Q-Q plot with the normal line\n",
    "qq_fig.add_trace(line_data)\n",
    "\n",
    "# Show the plots\n",
    "fig.show()\n",
    "qq_fig.show()\n",
    "\n",
    "## What questions do we want to ask of the data?\n",
    "1. Distribution of dwelling types and their relation to sale prices?\n",
    "2. Does zoning impact sale price?\n",
    "3. Does street and alley access types effect on sale price?\n",
    "4. What is the Average sale price by property shape?\n",
    "5. Is there a Correlation between Property Age and Sale Price\n",
    "6. Is there a Correlation between Living Area and Sale Price\n",
    "7. Does price change year to year?\n",
    "\n",
    "# 1. Distribution of dwelling types and their relation to sale prices\n",
    "dwelling_types = df['BldgType'].value_counts()\n",
    "dwelling_prices = df.groupby('BldgType')['SalePrice'].mean()\n",
    "\n",
    "# Format labels for the second graph\n",
    "formatted_dwelling_prices = ['$' + f'{value:,.2f}' for value in dwelling_prices.values]\n",
    "\n",
    "# Create bar chart for dwelling types\n",
    "fig1 = go.Figure(data=[go.Bar(\n",
    "    x=dwelling_types.index,\n",
    "    y=dwelling_types.values,\n",
    "    marker_color='rgb(76, 175, 80)',\n",
    "    text=dwelling_types.values,\n",
    "    textposition='outside',\n",
    "    cliponaxis=False,  # Allow text to render outside the chart\n",
    "    width=0.4,\n",
    "    marker=dict(line=dict(width=2, color='rgba(0,0,0,1)'), opacity=1)\n",
    ")])\n",
    "fig1.update_layout(\n",
    "    title='Distribution of Building Types',\n",
    "    xaxis_title='Building Type',\n",
    "    yaxis_title='Count',\n",
    "    plot_bgcolor='rgba(34, 34, 34, 1)',\n",
    "    paper_bgcolor='rgba(34, 34, 34, 1)',\n",
    "    font=dict(color='white'),\n",
    "    margin=dict(t=100, b=50, l=50, r=50),  # Increase top margin for better text visibility\n",
    "    yaxis=dict(automargin=True)\n",
    ")\n",
    "\n",
    "# Create bar chart for dwelling prices\n",
    "fig2 = go.Figure(data=[go.Bar(\n",
    "    x=dwelling_prices.index,\n",
    "    y=dwelling_prices.values,\n",
    "    marker_color='rgb(156, 39, 176)',\n",
    "    text=formatted_dwelling_prices,\n",
    "    textposition='outside',\n",
    "    cliponaxis=False,  # Allow text to render outside the chart\n",
    "    width=0.4,\n",
    "    marker=dict(line=dict(width=2, color='rgba(0,0,0,1)'), opacity=1)\n",
    ")])\n",
    "fig2.update_layout(\n",
    "    title='Average Sale Price by Building Type',\n",
    "    xaxis_title='Building Type',\n",
    "    yaxis_title='Price',\n",
    "    plot_bgcolor='rgba(34, 34, 34, 1)',\n",
    "    paper_bgcolor='rgba(34, 34, 34, 1)',\n",
    "    font=dict(color='white'),\n",
    "    margin=dict(t=100, b=50, l=50, r=50),  # Increase top margin for better text visibility\n",
    "    yaxis=dict(automargin=True)\n",
    ")\n",
    "\n",
    "# Show the figures\n",
    "fig1.show()\n",
    "fig2.show()\n",
    "\n",
    "# 2. Zoning impact on sale price\n",
    "zoning_prices = df.groupby('MSZoning')['SalePrice'].mean()\n",
    "fig3 = px.bar(x=zoning_prices.index, y=zoning_prices.values, title='Average Sale Price by Zoning',\n",
    "              color_discrete_sequence=['purple', 'green'], text=zoning_prices.values, \n",
    "              template='plotly_dark')\n",
    "\n",
    "fig3.update_traces(texttemplate='$%{text:,.0f}', textposition='outside', cliponaxis=False)\n",
    "fig3.update_yaxes(title='Sale Price', tickprefix='$', tickformat=',')\n",
    "fig3.update_xaxes(title='Zoning')\n",
    "fig3.update_layout(uniformtext_minsize=8, uniformtext_mode='hide')\n",
    "\n",
    "fig3.show()\n",
    "\n",
    "# 3. Street and alley access types effect on sale price\n",
    "street_prices = df.groupby('Street')['SalePrice'].mean()\n",
    "alley_prices = df.groupby('Alley')['SalePrice'].mean()\n",
    "\n",
    "# Street Prices\n",
    "colors_street = np.where(street_prices.index == 'Pave', 'purple', 'green')\n",
    "fig5 = px.bar(x=street_prices.index, y=street_prices.values, title='Average Sale Price by Street Type',\n",
    "              template='plotly_dark', text=street_prices.values,\n",
    "              color=colors_street, color_discrete_sequence=['purple', 'green'])\n",
    "\n",
    "fig5.update_traces(texttemplate='$%{text:,.0f}', textposition='outside', cliponaxis=False)\n",
    "fig5.update_yaxes(title='Sale Price', tickprefix='$', tickformat=',')\n",
    "fig5.update_xaxes(title='Street Type')\n",
    "fig5.update_layout(showlegend=False)\n",
    "\n",
    "# Alley Prices\n",
    "colors_alley = np.where(alley_prices.index == 'Pave', 'purple', 'green')\n",
    "fig6 = px.bar(x=alley_prices.index, y=alley_prices.values, title='Average Sale Price by Alley Type',\n",
    "              template='plotly_dark', text=alley_prices.values,\n",
    "              color=colors_alley, color_discrete_sequence=['purple', 'green'])\n",
    "\n",
    "fig6.update_traces(texttemplate='$%{text:,.0f}', textposition='outside', cliponaxis=False)\n",
    "fig6.update_yaxes(title='Sale Price', tickprefix='$', tickformat=',')\n",
    "fig6.update_xaxes(title='Alley Type')\n",
    "fig6.update_layout(showlegend=False)\n",
    "\n",
    "fig5.show()\n",
    "fig6.show()\n",
    "\n",
    "# 4. Average sale price by property shape\n",
    "colors = px.colors.qualitative.Plotly\n",
    "\n",
    "shape_prices = df.groupby('LotShape')['SalePrice'].mean()\n",
    "contour_prices = df.groupby('LandContour')['SalePrice'].mean()\n",
    "# Shape Prices\n",
    "fig7 = px.bar(x=shape_prices.index, y=shape_prices.values, title='Average Sale Price by Property Shape',\n",
    "              template='plotly_dark', text=shape_prices.values)\n",
    "\n",
    "fig7.update_traces(marker_color=colors, texttemplate='$%{text:,.0f}', textposition='outside',  cliponaxis=False)\n",
    "fig7.update_yaxes(title='Sale Price', tickprefix='$', tickformat=',')\n",
    "fig7.update_xaxes(title='Property Shape')\n",
    "fig7.update_layout(showlegend=False)\n",
    "\n",
    "# Contour Prices\n",
    "fig8 = px.bar(x=contour_prices.index, y=contour_prices.values, title='Average Sale Price by Property Contour',\n",
    "              template='plotly_dark', text=contour_prices.values)\n",
    "\n",
    "fig8.update_traces(marker_color=colors, texttemplate='$%{text:,.0f}', textposition='outside', cliponaxis=False)\n",
    "fig8.update_yaxes(title='Sale Price', tickprefix='$', tickformat=',')\n",
    "fig8.update_xaxes(title='Property Contour')\n",
    "fig8.update_layout(showlegend=False)\n",
    "\n",
    "fig7.show()\n",
    "fig8.show()\n",
    "\n",
    "# 5. Calculate Property Age\n",
    "df['PropertyAge'] = df['YrSold'] - df['YearBuilt']\n",
    "\n",
    "# Calculate Correlation between Property Age and Sale Price\n",
    "age_price_corr = df['PropertyAge'].corr(df['SalePrice'])\n",
    "print(f'Correlation between Property Age and Sale Price: {age_price_corr}')\n",
    "\n",
    "# Create a scatter plot to visualize the relationship between Property Age and Sale Price\n",
    "fig9 = px.scatter(df, x='PropertyAge', y='SalePrice', title='Property Age vs Sale Price', color='PropertyAge', color_continuous_scale=px.colors.sequential.Purp)\n",
    "\n",
    "fig9.update_layout(plot_bgcolor='rgb(30,30,30)', paper_bgcolor='rgb(30,30,30)', font=dict(color='white'))\n",
    "\n",
    "fig9.show()\n",
    "\n",
    "# 6. Calculate Correlation between Living Area and Sale Price\n",
    "living_area_price_corr = df['GrLivArea'].corr(df['SalePrice'])\n",
    "print(f'Correlation between Living Area (above grade) and Sale Price: {living_area_price_corr}')\n",
    "\n",
    "# Create a scatter plot to visualize the relationship between Living Area and Sale Price\n",
    "fig10 = px.scatter(df, x='GrLivArea', y='SalePrice', title='Living Area (above grade) vs Sale Price', color='GrLivArea', color_continuous_scale=px.colors.sequential.Purp)\n",
    "\n",
    "fig10.update_layout(plot_bgcolor='rgb(30,30,30)', paper_bgcolor='rgb(30,30,30)', font=dict(color='white'))\n",
    "\n",
    "fig10.show()\n",
    "\n",
    "# 7. Box plot of price over the years\n",
    "yearly_avg_sale_price = df.groupby('YrSold')['SalePrice'].mean()\n",
    "\n",
    "fig13 = px.box(df, x='YrSold', y='SalePrice', title='Sale Price Trends Over the Years',\n",
    "               points=False, color_discrete_sequence=['green'])\n",
    "\n",
    "fig13.add_trace(px.line(x=yearly_avg_sale_price.index, y=yearly_avg_sale_price.values).data[0])\n",
    "\n",
    "fig13.update_traces(line=dict(color='purple', width=4), selector=dict(type='scatter', mode='lines'))\n",
    "\n",
    "for year, avg_price in yearly_avg_sale_price.items():\n",
    "    fig13.add_annotation(\n",
    "        x=year,\n",
    "        y=avg_price,\n",
    "        text=f\"{avg_price:,.0f}\",\n",
    "        font=dict(color='white'),\n",
    "        showarrow=False,\n",
    "        bgcolor='rgba(128, 0, 128, 0.6)'\n",
    "    )\n",
    "\n",
    "fig13.update_layout(\n",
    "    plot_bgcolor='rgb(30,30,30)',\n",
    "    paper_bgcolor='rgb(30,30,30)',\n",
    "    font=dict(color='white'),\n",
    "    xaxis_title='Year Sold',\n",
    "    yaxis_title='Sale Price'\n",
    ")\n",
    "\n",
    "fig13.show()\n",
    "\n",
    "## Creating a Data Pipeline\n",
    "Why do this? - So we have consistent infrastructure for transforming the test set\n",
    "\n",
    "Goal - To create infrastructure that lets us make changes without breaking everything\n",
    "\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "\n",
    "\n",
    "# Define transformers for numerical and categorical columns\n",
    "numerical_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='mean')),\n",
    "    ('scaler', StandardScaler())\n",
    "])\n",
    "\n",
    "categorical_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='constant', fill_value='missing')),\n",
    "    ('onehot', OneHotEncoder(handle_unknown='ignore', sparse_output = False))\n",
    "])\n",
    "\n",
    "# Update categorical and numerical columns\n",
    "categorical_columns = df.select_dtypes(include=['object', 'category']).columns\n",
    "numerical_columns = df.select_dtypes(include=['int64', 'float64']).columns\n",
    "\n",
    "# Remove target variable from numerical columns\n",
    "numerical_columns = numerical_columns.drop('SalePrice')\n",
    "\n",
    "# Combine transformers using ColumnTransformer\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numerical_transformer, numerical_columns),\n",
    "        ('cat', categorical_transformer, categorical_columns)\n",
    "    ],remainder = 'passthrough')\n",
    "\n",
    "# Create a pipeline with the preprocessor\n",
    "pipeline = Pipeline(steps=[\n",
    "    ('preprocessor', preprocessor)])\n",
    "\n",
    "# Apply the pipeline to dataset\n",
    "X = df.drop('SalePrice', axis=1)\n",
    "y = np.log(df['SalePrice']) #normalize dependent variable \n",
    "X_preprocessed = pipeline.fit_transform(X)\n",
    "\n",
    "## Fit and Parameter Tune models\n",
    "We explore some different types of models here and see how they work (or don't work)\n",
    "\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.model_selection import GridSearchCV, KFold, cross_val_score\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_preprocessed, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Define the models\n",
    "models = {\n",
    "    'LinearRegression': LinearRegression(),\n",
    "    'RandomForest': RandomForestRegressor(random_state=42),\n",
    "    'XGBoost': XGBRegressor(random_state=42)\n",
    "}\n",
    "\n",
    "# Define the hyperparameter grids for each model\n",
    "param_grids = {\n",
    "    'LinearRegression': {},\n",
    "    'RandomForest': {\n",
    "        'n_estimators': [100, 200, 500],\n",
    "        'max_depth': [None, 10, 30],\n",
    "        'min_samples_split': [2, 5, 10],\n",
    "    },\n",
    "    'XGBoost': {\n",
    "        'n_estimators': [100, 200, 500],\n",
    "        'learning_rate': [0.01, 0.1, 0.3],\n",
    "        'max_depth': [3, 6, 10],\n",
    "    }\n",
    "}\n",
    "\n",
    "# 3-fold cross-validation\n",
    "cv = KFold(n_splits=3, shuffle=True, random_state=42)\n",
    "\n",
    "# Train and tune the models\n",
    "grids = {}\n",
    "for model_name, model in models.items():\n",
    "    #print(f'Training and tuning {model_name}...')\n",
    "    grids[model_name] = GridSearchCV(estimator=model, param_grid=param_grids[model_name], cv=cv, scoring='neg_mean_squared_error', n_jobs=-1, verbose=2)\n",
    "    grids[model_name].fit(X_train, y_train)\n",
    "    best_params = grids[model_name].best_params_\n",
    "    best_score = np.sqrt(-1 * grids[model_name].best_score_)\n",
    "    \n",
    "    print(f'Best parameters for {model_name}: {best_params}')\n",
    "    print(f'Best RMSE for {model_name}: {best_score}\\n')\n",
    "\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "\n",
    "X_train_scaled = X_train.copy()\n",
    "X_test_scaled = X_test.copy()\n",
    "\n",
    "# Create an MLPRegressor instance\n",
    "mlp = MLPRegressor(random_state=42,max_iter=10000, n_iter_no_change = 3,learning_rate_init=0.001)\n",
    "\n",
    "# Define the parameter grid for tuning\n",
    "param_grid = {\n",
    "    'hidden_layer_sizes': [(10,), (10,10), (10,10,10), (25)],\n",
    "    'activation': ['relu', 'tanh'],\n",
    "    'solver': ['adam'],\n",
    "    'alpha': [0.0001, 0.001, 0.01],\n",
    "    'learning_rate': ['constant', 'invscaling', 'adaptive'],\n",
    "}\n",
    "\n",
    "# Create the GridSearchCV object\n",
    "grid_search_mlp = GridSearchCV(mlp, param_grid, scoring='neg_mean_squared_error', cv=3, n_jobs=-1, verbose=1)\n",
    "\n",
    "# Fit the model on the training data\n",
    "grid_search_mlp.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Print the best parameters found during the search\n",
    "print(\"Best parameters found: \", grid_search_mlp.best_params_)\n",
    "\n",
    "# Evaluate the model on the test data\n",
    "best_score = np.sqrt(-1 * grid_search_mlp.best_score_)\n",
    "print(\"Test score: \", best_score)\n",
    "\n",
    "## Principal Component Analysis\n",
    "Basic Feature Engineering\n",
    "\n",
    "#pca\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "pca = PCA()\n",
    "X_pca_pre = pca.fit_transform(X_preprocessed)\n",
    "\n",
    "# Calculate the cumulative explained variance\n",
    "cumulative_explained_variance = np.cumsum(pca.explained_variance_ratio_)\n",
    "\n",
    "# Choose the number of components based on the explained variance threshold\n",
    "n_components = np.argmax(cumulative_explained_variance >= 0.95) + 1\n",
    "\n",
    "pca = PCA(n_components=n_components)\n",
    "pipeline_pca = Pipeline(steps=\n",
    "                        [('preprocessor', preprocessor),\n",
    "                        ('pca', pca)])\n",
    "\n",
    "X_pca = pipeline_pca.fit_transform(X)\n",
    "\n",
    "## Running the same models with new data\n",
    "\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.model_selection import GridSearchCV, KFold, cross_val_score\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train_pca, X_test_pca, y_train_pca, y_test_pca = train_test_split(X_pca, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Define the models\n",
    "models = {\n",
    "    'LinearRegression': LinearRegression(),\n",
    "    'RandomForest': RandomForestRegressor(random_state=42),\n",
    "    'XGBoost': XGBRegressor(random_state=42)\n",
    "}\n",
    "\n",
    "# Define the hyperparameter grids for each model\n",
    "param_grids = {\n",
    "    'LinearRegression': {},\n",
    "    'RandomForest': {'n_estimators': [100, 200, 500],\n",
    "        'max_depth': [None, 10, 30],\n",
    "        'min_samples_split': [2, 5, 10],\n",
    "    },\n",
    "    'XGBoost': {\n",
    "        'n_estimators': [100, 200, 500],\n",
    "        'learning_rate': [0.01, 0.1, 0.3],\n",
    "        'max_depth': [3, 6, 10],\n",
    "    }\n",
    "}\n",
    "\n",
    "# 3-fold cross-validation\n",
    "cv = KFold(n_splits=3, shuffle=True, random_state=42)\n",
    "\n",
    "# Train and tune the models\n",
    "grids_pca = {}\n",
    "for model_name, model in models.items():\n",
    "    #print(f'Training and tuning {model_name}...')\n",
    "    grids_pca[model_name] = GridSearchCV(estimator=model, param_grid=param_grids[model_name], cv=cv, scoring='neg_mean_squared_error', n_jobs=-1, verbose=2)\n",
    "    grids_pca[model_name].fit(X_train_pca, y_train_pca)\n",
    "    best_params = grids_pca[model_name].best_params_\n",
    "    best_score = np.sqrt(-1 * grids_pca[model_name].best_score_)\n",
    "    \n",
    "    print(f'Best parameters for {model_name}: {best_params}')\n",
    "    print(f'Best RMSE for {model_name}: {best_score}\\n')\n",
    "\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "\n",
    "X_train_scaled_pca = X_train_pca.copy()\n",
    "X_test_scaled_pca = X_test_pca.copy()\n",
    "\n",
    "# Create an MLPRegressor instance\n",
    "mlp = MLPRegressor(random_state=42,max_iter=10000, n_iter_no_change = 3,learning_rate_init=0.001)\n",
    "\n",
    "# Define the parameter grid for tuning\n",
    "param_grid = {\n",
    "    'hidden_layer_sizes': [(10,), (10,10), (10,10,10), (25)],\n",
    "    'activation': ['relu', 'tanh'],\n",
    "    'solver': ['adam'],\n",
    "    'alpha': [0.0001, 0.001, 0.01, .1, 1],\n",
    "    'learning_rate': ['constant', 'invscaling', 'adaptive'],\n",
    "}\n",
    "\n",
    "# Create the GridSearchCV object\n",
    "grid_search_mlp_pca = GridSearchCV(mlp, param_grid, scoring='neg_mean_squared_error', cv=3, n_jobs=-1, verbose=1)\n",
    "\n",
    "# Fit the model on the training data\n",
    "grid_search_mlp_pca.fit(X_train_scaled_pca, y_train)\n",
    "\n",
    "# Print the best parameters found during the search\n",
    "print(\"Best parameters found: \", grid_search_mlp_pca.best_params_)\n",
    "\n",
    "# Evaluate the model on the test data\n",
    "best_score = np.sqrt(-1 * grid_search_mlp_pca.best_score_)\n",
    "print(\"Test score: \", best_score)\n",
    "\n",
    "from sklearn.metrics import mean_squared_error\n",
    "for i in grids.keys():\n",
    "    print (i + ': ' + str(np.sqrt(mean_squared_error(grids[i].predict(X_test), y_test))))\n",
    "\n",
    "from sklearn.metrics import mean_squared_error\n",
    "for i in grids.keys():\n",
    "    print (i + ': ' + str(np.sqrt(mean_squared_error(grids_pca[i].predict(X_test_pca), y_test))))\n",
    "\n",
    "print( str(np.sqrt(mean_squared_error(grid_search_mlp.predict(X_test_scaled),y_test))))\n",
    "\n",
    "print( str(np.sqrt(mean_squared_error(grid_search_mlp_pca.predict(X_test_scaled_pca),y_test))))\n",
    "\n",
    "var_explore = df[['Fence','Alley','MiscFeature','PoolQC','FireplaceQu','GarageCond','GarageQual','GarageFinish','GarageType','BsmtExposure','BsmtFinType2','BsmtFinType1','BsmtCond','BsmtQual','MasVnrType','Electrical','MSZoning','Utilities','Exterior1st','Exterior2nd','KitchenQual','Functional','SaleType','LotFrontage','GarageYrBlt','MasVnrArea','BsmtFullBath','BsmtHalfBath','GarageCars','GarageArea','TotalBsmtSF']]\n",
    "\n",
    "display(HTML(create_scrollable_table(var_explore, 'var_explore', 'List of Variables to Explore for Feature Engineering')))\n",
    "\n",
    "from sklearn.preprocessing import FunctionTransformer\n",
    "\n",
    "# feature engineering functions \n",
    "def custom_features(df):\n",
    "    df_out = df.copy()\n",
    "    df_out['PropertyAge'] = df_out['YrSold'] - df_out['YearBuilt']\n",
    "    df_out['TotalSF'] = df_out['TotalBsmtSF'] + df_out['1stFlrSF'] + df_out['2ndFlrSF']\n",
    "    df_out['TotalBath'] = df_out['FullBath'] + 0.5 * df_out['HalfBath'] + df_out['BsmtFullBath'] + 0.5 * df['BsmtHalfBath']\n",
    "    df_out['HasRemodeled'] = (df_out['YearRemodAdd'] != df_out['YearBuilt']).astype(object)\n",
    "    df_out['Has2ndFloor'] = (df_out['2ndFlrSF'] > 0).astype(object)\n",
    "    df_out['HasGarage'] = (df_out['GarageArea'] > 0).astype(object)\n",
    "    df_out['YrSold_cat'] = df_out['YrSold'].astype(object)\n",
    "    df_out['MoSold_cat'] = df_out['MoSold'].astype(object)\n",
    "    df_out['YearBuilt_cat'] = df_out['YearBuilt'].astype(object)\n",
    "    df_out['MSSubClass_cat'] = df_out['MSSubClass'].astype(object)\n",
    "    \n",
    "    return df_out\n",
    "\n",
    "feature_engineering_transformer = FunctionTransformer(custom_features)\n",
    "\n",
    "# Identify categorical and numerical columns\n",
    "new_cols_categorical = pd.Index(['HasRemodeled', 'Has2ndFloor', 'HasGarage'])\n",
    "new_cols_numeric = pd.Index(['PropertyAge', 'TotalSF', 'TotalBath', 'YrSold_cat', 'MoSold_cat', 'YearBuilt_cat', 'MSSubClass_cat'])\n",
    "\n",
    "# Update categorical and numerical columns\n",
    "categorical_columns = df.select_dtypes(include=['object', 'category']).columns.append(new_cols_categorical)\n",
    "numerical_columns = df.select_dtypes(include=['int64', 'float64']).columns.append(new_cols_numeric)\n",
    "\n",
    "# Remove target variable from numerical columns\n",
    "numerical_columns = numerical_columns.drop('SalePrice')\n",
    "\n",
    "# Combine transformers using ColumnTransformer\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numerical_transformer, numerical_columns),\n",
    "        ('cat', categorical_transformer, categorical_columns)\n",
    "    ],remainder = 'passthrough')\n",
    "\n",
    "# Create a pipeline with the preprocessor\n",
    "pipeline_fe = Pipeline(steps=[\n",
    "    ('fe', feature_engineering_transformer),\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('pca', pca)])\n",
    "\n",
    "# Apply the pipeline to your dataset\n",
    "X = df.drop('SalePrice', axis=1)\n",
    "y = np.log(df['SalePrice'])\n",
    "X_preprocessed_fe = pipeline_fe.fit_transform(X)\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train_fe, X_test_fe, y_train_fe, y_test_fe = train_test_split(X_preprocessed_fe, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Define the models\n",
    "models = {\n",
    "    'LinearRegression': LinearRegression(),\n",
    "    'RandomForest': RandomForestRegressor(random_state=42),\n",
    "    'XGBoost': XGBRegressor(random_state=42)\n",
    "}\n",
    "\n",
    "# Define the hyperparameter grids for each model\n",
    "param_grids = {\n",
    "    'LinearRegression': {},\n",
    "    'RandomForest': {\n",
    "        'n_estimators': [100, 200, 500],\n",
    "        'max_depth': [None, 10, 30],\n",
    "        'min_samples_split': [2, 5, 10],\n",
    "    },\n",
    "    'XGBoost': {\n",
    "        'n_estimators': [100, 200, 500],\n",
    "        'learning_rate': [0.01, 0.1, 0.3],\n",
    "        'max_depth': [3, 6, 10],\n",
    "    }\n",
    "}\n",
    "\n",
    "# 3-fold cross-validation\n",
    "cv = KFold(n_splits=3, shuffle=True, random_state=42)\n",
    "\n",
    "# Train and tune the models\n",
    "grids_fe = {}\n",
    "for model_name, model in models.items():\n",
    "    #print(f'Training and tuning {model_name}...')\n",
    "    grids_fe[model_name] = GridSearchCV(estimator=model, param_grid=param_grids[model_name], cv=cv, scoring='neg_mean_squared_error', n_jobs=-1, verbose=2)\n",
    "    grids_fe[model_name].fit(X_train_fe, y_train_fe)\n",
    "    best_params = grids_fe[model_name].best_params_\n",
    "    best_score = np.sqrt(-1 * grids_fe[model_name].best_score_)\n",
    "    \n",
    "    print(f'Best parameters for {model_name}: {best_params}')\n",
    "    print(f'Best RMSE for {model_name}: {best_score}\\n')\n",
    "\n",
    "X_train_scaled_fe = X_train_fe.copy()\n",
    "X_test_scaled_fe = X_test_fe.copy()\n",
    "\n",
    "# Create an MLPRegressor instance\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "\n",
    "mlp = MLPRegressor(random_state=42, max_iter=10000, n_iter_no_change=3)\n",
    "\n",
    "# Define the parameter grid for tuning\n",
    "param_grid = {\n",
    "    'hidden_layer_sizes': [(10,), (10, 10), (10, 25)],\n",
    "    'activation': ['relu', 'tanh', 'sigmoid'],\n",
    "    'solver': ['adam', 'sgd'],\n",
    "    'alpha': [.1, .5, 1, 10, 100],\n",
    "    'learning_rate': ['constant', 'invscaling', 'adaptive'],\n",
    "    'learning_rate_init' : [0.1]\n",
    "}\n",
    "\n",
    "# Create the GridSearchCV object\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "grid_search_mlp_fe = GridSearchCV(mlp, param_grid, scoring='neg_mean_squared_error', cv=3, n_jobs=-1, verbose=1)\n",
    "\n",
    "# Fit the model on the training data\n",
    "grid_search_mlp_fe.fit(X_train_scaled_fe, y_train_fe)\n",
    "\n",
    "# Print the best parameters found during the search\n",
    "print(\"Best parameters found: \", grid_search_mlp_fe.best_params_)\n",
    "\n",
    "# Evaluate the model on the test data\n",
    "best_score = np.sqrt(-1 * grid_search_mlp_fe.best_score_)\n",
    "print(\"Test score: \", best_score)\n",
    "\n",
    "from sklearn.metrics import mean_squared_error\n",
    "for i in grids.keys():\n",
    "    print (i + ': ' + str(np.sqrt(mean_squared_error(grids_fe[i].predict(X_test_fe), y_test))))\n",
    "\n",
    "print( str(np.sqrt(mean_squared_error(grid_search_mlp_fe.predict(X_test_scaled_fe),y_test))))\n",
    "\n",
    "## Running on the test set\n",
    "\n",
    "df_test = pd.read_csv('/Users/achalas/Documents/Housing Prices Project/house-prices-advanced-regression-techniques/test.csv')\n",
    "\n",
    "df_test_preprocessed = pipeline_fe.transform(df_test)\n",
    "\n",
    "### XGBoost Submission\n",
    "This block generates the submission file using the predictions from the XGBoost model. The predictions are exponentiated to reverse the earlier log-transformation and then saved as a CSV file for submission.\n",
    "\n",
    "\n",
    "y_xgboost = np.exp(grids_fe['XGBoost'].predict(df_test_preprocessed))\n",
    "\n",
    "df_xgboost_out = df_test[['Id']].copy()\n",
    "df_xgboost_out['SalePrice'] = y_xgboost\n",
    "\n",
    "#\n",
    "df_xgboost_out.to_csv('submission_xgboost_new_features_normalized.csv', index=False)\n",
    "\n",
    "### Random Forest Submission\n",
    "This block generates the submission file using the predictions from the Random Forest model. The predictions are exponentiated to reverse the earlier log-transformation applied to SalePrice, and the results are saved as a CSV file for submission.\n",
    "\n",
    "\n",
    "\n",
    "y_rf = np.exp(grids_fe['RandomForest'].predict(df_test_preprocessed))\n",
    "\n",
    "df_rf_out = df_test[['Id']].copy()\n",
    "df_rf_out['SalePrice'] = y_rf\n",
    "\n",
    "#\n",
    "df_rf_out.to_csv('submission_rf_normalized.csv', index=False)\n",
    "\n",
    "### MLP (Neural Network) Submission\n",
    "This block generates the submission file using the predictions from the MLP (Neural Network) model. As with other models, the predictions are exponentiated to reverse the log-transformation of SalePrice. The results are saved as a CSV file for submission.\n",
    "\n",
    "y_mlp = np.exp(grid_search_mlp_fe.predict(df_test_preprocessed))\n",
    "\n",
    "df_mlp_out = df_test[['Id']].copy()\n",
    "df_mlp_out['SalePrice'] = y_mlp\n",
    "\n",
    "df_mlp_out.to_csv('submission_mlp_normalized.csv', index=False)\n",
    "\n",
    "### Averaged Ensemble Submission\n",
    "This block generates the submission file using the predictions from the averaged ensemble. The predictions from Random Forest, XGBoost, and MLP models are averaged to produce a single prediction for each row. This approach leverages the strengths of all three models for better generalization.\n",
    "\n",
    "y_avg_ens = (y_rf + y_xgboost + y_mlp)/3\n",
    "\n",
    "df_avg_ens_out = df_test[['Id']].copy()\n",
    "df_avg_ens_out['SalePrice'] = y_avg_ens\n",
    "\n",
    "#\n",
    "df_avg_ens_out.to_csv('submission_avg_ens_new_features_normalized.csv', index=False)\n",
    "\n",
    "from sklearn.ensemble import StackingRegressor\n",
    "\n",
    "grids_fe['MLP'] =   grid_search_mlp_fe\n",
    "\n",
    "best_estimators = [(model_name, grid.best_estimator_) for model_name, grid in grids_fe.items()]\n",
    "\n",
    "# Define the candidate meta-models\n",
    "meta_models = {\n",
    "    'MLP': MLPRegressor(random_state=42, max_iter=10000, n_iter_no_change=3, learning_rate_init=0.001),\n",
    "    'LinearRegression': LinearRegression(),\n",
    "    'XGBoost': XGBRegressor(random_state=42)\n",
    "}\n",
    "\n",
    "# Define the hyperparameter grids for each meta-model\n",
    "meta_param_grids = {\n",
    "    'MLP': {\n",
    "        'final_estimator__hidden_layer_sizes': [(10,), (10, 10)],\n",
    "        'final_estimator__activation': ['relu', 'tanh'],\n",
    "        'final_estimator__solver': ['adam', 'sgd'],\n",
    "        'final_estimator__alpha': [ 0.001, 0.01, .1, .5],\n",
    "        'final_estimator__learning_rate': ['constant', 'invscaling', 'adaptive'],\n",
    "    },\n",
    "    'LinearRegression': {},\n",
    "    'XGBoost': {\n",
    "        'final_estimator__n_estimators': [100, 200, 500],\n",
    "        'final_estimator__learning_rate': [0.01, 0.1, 0.3],\n",
    "        'final_estimator__max_depth': [3, 6, 10],\n",
    "    }\n",
    "}\n",
    "\n",
    "# 3-fold cross-validation\n",
    "cv = KFold(n_splits=3, shuffle=True, random_state=42)\n",
    "\n",
    "# Train and tune the stacking ensemble\n",
    "best_score = float('inf')\n",
    "best_model = None\n",
    "\n",
    "for meta_name, meta_model in meta_models.items():\n",
    "    print(f'Training and tuning {meta_name} as the meta-model...')\n",
    "    stacking_regressor = StackingRegressor(estimators=best_estimators, final_estimator=meta_model, cv=cv)\n",
    "    grid_search = GridSearchCV(estimator=stacking_regressor, param_grid=meta_param_grids[meta_name], cv=cv, scoring='neg_mean_squared_error', n_jobs=-1, verbose=1)\n",
    "    grid_search.fit(X_train_fe, y_train_fe)\n",
    "    best_params = grid_search.best_params_\n",
    "    best_rmse = np.sqrt(-1 * grid_search.best_score_)\n",
    "    \n",
    "    print(f'Best parameters for {meta_name}: {best_params}')\n",
    "    print(f'Best RMSE for {meta_name}: {best_rmse}\\n')\n",
    "    \n",
    "    if best_rmse < best_score:\n",
    "        best_score = best_rmse\n",
    "        best_model = grid_search\n",
    "\n",
    "# Evaluate the best stacking ensemble on the test data\n",
    "y_pred = best_model.predict(X_test_fe)\n",
    "rmse = np.sqrt(mean_squared_error(y_test_fe, y_pred))\n",
    "print(f\"Best stacking ensemble's RMSE on test data: {rmse}\")\n",
    "\n",
    "### Stacking Ensemble Submission\n",
    "This block generates the submission file using the predictions from the stacking ensemble. The stacking regressor combines the predictions of multiple models (Random Forest, XGBoost, MLP, etc.) using a meta-model to create a single, more accurate prediction for each row.\n",
    "\n",
    "\n",
    "y_stack = np.exp(best_model.predict(df_test_preprocessed))\n",
    "\n",
    "df_stack_out = df_test[['Id']].copy()\n",
    "df_stack_out['SalePrice'] = y_stack\n",
    "\n",
    "df_stack_out.to_csv('submission_stack_new_features_normalized.csv', index=False)\n",
    "\n",
    "df_stack_out.to_csv('submission.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
